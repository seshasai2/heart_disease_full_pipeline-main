{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1043970,"sourceType":"datasetVersion","datasetId":576697}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:31:32.435269Z","iopub.execute_input":"2025-10-22T03:31:32.435541Z","iopub.status.idle":"2025-10-22T03:31:33.818443Z","shell.execute_reply.started":"2025-10-22T03:31:32.435515Z","shell.execute_reply":"2025-10-22T03:31:33.817632Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv\")\nprint(df.columns)\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:41:47.565017Z","iopub.execute_input":"2025-10-22T03:41:47.565380Z","iopub.status.idle":"2025-10-22T03:41:47.589345Z","shell.execute_reply.started":"2025-10-22T03:41:47.565353Z","shell.execute_reply":"2025-10-22T03:41:47.588383Z"}},"outputs":[{"name":"stdout","text":"Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'condition'],\n      dtype='object')\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n0   69    1   0       160   234    1        2      131      0      0.1      1   \n1   69    0   0       140   239    0        0      151      0      1.8      0   \n2   66    0   0       150   226    0        0      114      0      2.6      2   \n3   65    1   0       138   282    1        2      174      0      1.4      1   \n4   64    1   0       110   211    0        2      144      1      1.8      1   \n\n   ca  thal  condition  \n0   1     0          0  \n1   2     0          0  \n2   0     0          0  \n3   1     0          1  \n4   0     0          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>condition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69</td>\n      <td>1</td>\n      <td>0</td>\n      <td>160</td>\n      <td>234</td>\n      <td>1</td>\n      <td>2</td>\n      <td>131</td>\n      <td>0</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>69</td>\n      <td>0</td>\n      <td>0</td>\n      <td>140</td>\n      <td>239</td>\n      <td>0</td>\n      <td>0</td>\n      <td>151</td>\n      <td>0</td>\n      <td>1.8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66</td>\n      <td>0</td>\n      <td>0</td>\n      <td>150</td>\n      <td>226</td>\n      <td>0</td>\n      <td>0</td>\n      <td>114</td>\n      <td>0</td>\n      <td>2.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>65</td>\n      <td>1</td>\n      <td>0</td>\n      <td>138</td>\n      <td>282</td>\n      <td>1</td>\n      <td>2</td>\n      <td>174</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64</td>\n      <td>1</td>\n      <td>0</td>\n      <td>110</td>\n      <td>211</td>\n      <td>0</td>\n      <td>2</td>\n      <td>144</td>\n      <td>1</td>\n      <td>1.8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Full Kaggle-ready Heart Disease Pipeline (single file)\n# - Auto target detection & rename\n# - Robust FE + per-fold transforms (no leakage)\n# - XGBoost / LightGBM / CatBoost ensemble (safe fallbacks)\n# - Stratified K-Fold OOF + final full-data retrain\n# - Robust submission builder that uses test.csv or sample_submission if available\n# - Saves artifacts to ./artifacts_kaggle\n#\n# Copy-paste into a Kaggle notebook cell and run. Adjust TRAIN_PATH / TEST_PATH if needed.\n\nimport os, gc, glob, warnings, json\nfrom datetime import datetime\nwarnings.filterwarnings(\"ignore\")\nSEED = 42\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\n\n# Optional ML libraries detection\nHAS_XGB = HAS_LGB = HAS_CAT = HAS_OPTUNA = False\ntry:\n    import xgboost as xgb; HAS_XGB = True\nexcept Exception:\n    pass\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept Exception:\n    pass\ntry:\n    import catboost as cat; HAS_CAT = True\nexcept Exception:\n    pass\ntry:\n    import optuna; HAS_OPTUNA = True\nexcept Exception:\n    pass\n\nnp.random.seed(SEED)\n\n# -------------------- CONFIG --------------------\n# Adjust paths if your files are placed differently\nTRAIN_PATH = \"/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv\"\n# You may not have a separate test file. If present, set TEST_PATH; else sample_submission will be used.\nTEST_PATH = \"/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload_test.csv\"\nINPUT_ROOT = \"/kaggle/input/heart-disease-cleveland-uci\"\nOUT_DIR = \"artifacts_kaggle\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# Ensemble weights (will normalize automatically if some libs missing)\nWEIGHT_XGB = 0.5\nWEIGHT_LGB = 0.3\nWEIGHT_CAT = 0.2\n\nNFOLD = 10\nEARLY_STOP = 50\nMAX_ROUNDS = 2000\n\n# Submission config\nTARGET_COL = \"target\"\nID_COL_CANDIDATES = ['id','ID','Id','patient_id','PatientID','index']\nAS_INTEGER_SUBMISSION = False  # set True if competition expects 0/1 labels\n# ------------------------------------------------\n\ndef safe_read_csv(path):\n    if path is None:\n        return None\n    if os.path.exists(path):\n        return pd.read_csv(path)\n    return None\n\ndef save_submission_df(df_sub, filename=\"submission_final.csv\"):\n    out_path = os.path.join(OUT_DIR, filename)\n    df_sub.to_csv(out_path, index=False)\n    print(\"Saved submission to:\", out_path)\n    return out_path\n\n# -------------------- Load Data --------------------\ndf = safe_read_csv(TRAIN_PATH)\nif df is None:\n    raise FileNotFoundError(f\"Train file not found. Update TRAIN_PATH: {TRAIN_PATH}\")\n\ndf_test = safe_read_csv(TEST_PATH)  # may be None\n# also try to find sample_submission if test missing later\n\n# -------------------- Auto-detect target --------------------\npossible_targets = [\"target\", \"num\", \"condition\", \"HeartDisease\", \"diagnosis\"]\ntarget_col = None\nfor c in possible_targets:\n    if c in df.columns:\n        target_col = c; break\nif target_col is None:\n    for c in df.columns:\n        if df[c].dtype.kind in \"iu\" and df[c].nunique() <= 6 and c.lower() not in ['age','sex','chol','trestbps','thalach','ca','thal']:\n            target_col = c\n            print(f\"Auto-heuristic selected '{c}' as target; please verify.\")\n            break\nif target_col is None:\n    raise ValueError(f\"No target column found. Columns: {df.columns.tolist()}\")\n\nif target_col != TARGET_COL:\n    df = df.rename(columns={target_col: TARGET_COL})\n    print(f\"Renamed target column: {target_col} -> {TARGET_COL}\")\n\n# Convert multiclass -> binary if needed\nif df[TARGET_COL].nunique() > 2:\n    df[TARGET_COL] = (df[TARGET_COL] > 0).astype(int)\n\n# -------------------- Basic cleaning & FE --------------------\ndef safe_numeric_convert(d):\n    d = d.copy()\n    for c in d.columns:\n        if d[c].dtype == object:\n            try: d[c] = pd.to_numeric(d[c])\n            except: pass\n    return d\n\ndef feature_engineer(d):\n    d = d.copy()\n    if all(c in d.columns for c in [\"age\",\"chol\"]):\n        d[\"age_chol\"] = d[\"age\"] * (d[\"chol\"] + 1e-6)\n    if all(c in d.columns for c in [\"age\",\"thalach\"]):\n        d[\"age_thalach\"] = d[\"age\"] * (d[\"thalach\"] + 1e-6)\n    if all(c in d.columns for c in [\"chol\",\"trestbps\"]):\n        d[\"chol_trestbps_ratio\"] = d[\"chol\"] / (d[\"trestbps\"] + 1e-6)\n    if \"age\" in d.columns:\n        d[\"age_bin\"] = pd.cut(d[\"age\"], bins=[0,40,50,60,70,120], labels=False)\n    for c in [\"ca\",\"thal\"]:\n        if c in d.columns:\n            d[f\"{c}_missing\"] = d[c].isna().astype(int)\n    return d\n\ndf = safe_numeric_convert(df)\ndf = feature_engineer(df)\nif df_test is not None:\n    df_test = safe_numeric_convert(df_test)\n    df_test = feature_engineer(df_test)\n\n# -------------------- Features list & detect types --------------------\nfeatures = [c for c in df.columns if c != TARGET_COL]\nif len(features) == 0:\n    raise RuntimeError(\"No features found after excluding target.\")\n\ndef detect_cats(d, thresh=8):\n    cats = []\n    for c in d.columns:\n        if d[c].dtype.kind in \"iu\" and d[c].nunique() <= thresh:\n            cats.append(c)\n        elif d[c].dtype == object:\n            cats.append(c)\n    return cats\n\ncategorical_feats = detect_cats(df[features])\nnumeric_feats = [c for c in features if c not in categorical_feats]\n\n# ensure minimal numeric feature\nif len(numeric_feats) == 0:\n    df[\"_dummy_num\"] = 0.0\n    if df_test is not None:\n        df_test[\"_dummy_num\"] = 0.0\n    numeric_feats = [\"_dummy_num\"]\n    features.append(\"_dummy_num\")\n\nprint(\"Numeric features:\", numeric_feats)\nprint(\"Categorical features:\", categorical_feats)\n\n# -------------------- Per-fold transform (no leakage) --------------------\ndef transform_local(X_tr, X_val, X_test=None):\n    num_imp = SimpleImputer(strategy=\"median\")\n    scaler = StandardScaler()\n    ord_enc = None\n    if len(categorical_feats) > 0:\n        ord_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n\n    # numeric\n    Xn_tr = pd.DataFrame(num_imp.fit_transform(X_tr[numeric_feats]), columns=numeric_feats, index=X_tr.index)\n    Xn_val = pd.DataFrame(num_imp.transform(X_val[numeric_feats]), columns=numeric_feats, index=X_val.index)\n    Xn_test = None\n    if X_test is not None:\n        Xn_test = pd.DataFrame(num_imp.transform(X_test[numeric_feats]), columns=numeric_feats, index=X_test.index)\n\n    Xn_tr[numeric_feats] = scaler.fit_transform(Xn_tr[numeric_feats])\n    Xn_val[numeric_feats] = scaler.transform(Xn_val[numeric_feats])\n    if Xn_test is not None:\n        Xn_test[numeric_feats] = scaler.transform(Xn_test[numeric_feats])\n\n    # categorical\n    if ord_enc is not None:\n        Xc_tr = pd.DataFrame(ord_enc.fit_transform(X_tr[categorical_feats]), columns=categorical_feats, index=X_tr.index)\n        Xc_val = pd.DataFrame(ord_enc.transform(X_val[categorical_feats]), columns=categorical_feats, index=X_val.index)\n        Xc_test = pd.DataFrame(ord_enc.transform(X_test[categorical_feats]), columns=categorical_feats, index=X_test.index) if X_test is not None else None\n    else:\n        Xc_tr = pd.DataFrame(index=X_tr.index)\n        Xc_val = pd.DataFrame(index=X_val.index)\n        Xc_test = None\n\n    X_tr_t = pd.concat([Xn_tr.reset_index(drop=True), Xc_tr.reset_index(drop=True)], axis=1)\n    X_val_t = pd.concat([Xn_val.reset_index(drop=True), Xc_val.reset_index(drop=True)], axis=1)\n    X_test_t = pd.concat([Xn_test.reset_index(drop=True), Xc_test.reset_index(drop=True)], axis=1) if Xn_test is not None else None\n\n    return X_tr_t, X_val_t, X_test_t, {\"num_imp\": num_imp, \"scaler\": scaler, \"ord_enc\": ord_enc}\n\n# -------------------- Model wrappers --------------------\ndef train_xgb(X_tr, y_tr, X_val, y_val, params=None):\n    if not HAS_XGB:\n        raise RuntimeError(\"XGBoost not installed.\")\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dval = xgb.DMatrix(X_val, label=y_val)\n    default = {\"objective\":\"binary:logistic\",\"eval_metric\":\"auc\",\"seed\":SEED,\"verbosity\":0,\"nthread\":4,\"tree_method\":\"hist\"}\n    if params: default.update(params)\n    bst = xgb.train(default, dtrain, num_boost_round=MAX_ROUNDS, evals=[(dtrain,\"train\"),(dval,\"valid\")],\n                    early_stopping_rounds=EARLY_STOP, verbose_eval=False)\n    return bst\n\ndef train_lgb(X_tr, y_tr, X_val, y_val, params=None):\n    if not HAS_LGB:\n        raise RuntimeError(\"LightGBM not installed.\")\n    ltrain = lgb.Dataset(X_tr, label=y_tr)\n    lval = lgb.Dataset(X_val, label=y_val, reference=ltrain)\n    default = {\"objective\":\"binary\",\"metric\":\"auc\",\"verbosity\":-1,\"seed\":SEED,\"n_jobs\":4}\n    if params: default.update(params)\n    try:\n        from lightgbm import early_stopping, log_evaluation\n        bst = lgb.train(default, ltrain, num_boost_round=MAX_ROUNDS, valid_sets=[ltrain,lval],\n                        callbacks=[early_stopping(stopping_rounds=EARLY_STOP), log_evaluation(0)])\n    except Exception:\n        bst = lgb.train(default, ltrain, num_boost_round=MAX_ROUNDS, valid_sets=[ltrain,lval],\n                        early_stopping_rounds=EARLY_STOP, verbose_eval=False)\n    return bst\n\ndef train_cat(X_tr, y_tr, X_val, y_val, params=None):\n    if not HAS_CAT:\n        raise RuntimeError(\"CatBoost not installed.\")\n    default = {\"iterations\":MAX_ROUNDS,\"learning_rate\":0.03,\"depth\":6,\"loss_function\":\"Logloss\",\"eval_metric\":\"AUC\",\"random_seed\":SEED,\"verbose\":False}\n    if params: default.update(params)\n    model = cat.CatBoostClassifier(**default)\n    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=EARLY_STOP, verbose=False)\n    return model\n\n# default params for models\nxgb_default = {\"eta\":0.02,\"max_depth\":4,\"subsample\":0.8,\"colsample_bytree\":0.8}\nlgb_default = {\"learning_rate\":0.02,\"num_leaves\":31,\"feature_fraction\":0.8,\"bagging_fraction\":0.8,\"bagging_freq\":5}\ncat_default = {\"learning_rate\":0.02,\"depth\":5}\n\n# optional light tuning with optuna for fold-1\ndef optuna_light_xgb(X_tr, y_tr, X_val, y_val, n_trials=12):\n    if not (HAS_OPTUNA and HAS_XGB):\n        return xgb_default\n    def objective(trial):\n        params = {\n            \"eta\": trial.suggest_float(\"eta\", 0.005, 0.1, log=True),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        }\n        m = train_xgb(X_tr, y_tr, X_val, y_val, params=params)\n        preds = m.predict(xgb.DMatrix(X_val))\n        return roc_auc_score(y_val, preds)\n    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n    print(\"Optuna best params:\", study.best_params)\n    return study.best_params\n\n# -------------------- CV OOF training --------------------\nX = df[features].reset_index(drop=True)\ny = df[TARGET_COL].reset_index(drop=True)\n\nskf = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=SEED)\noof = np.zeros(len(X), dtype=float)\ntest_fold_preds = []\ncv_scores = []\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n    print(f\"\\n=== Fold {fold}/{NFOLD} ===\")\n    X_tr, X_val = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(drop=True)\n    y_tr, y_val = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(drop=True)\n\n    X_tr_t, X_val_t, X_test_t, fitted_objs = transform_local(X_tr, X_val, X_test=df_test[features] if df_test is not None else None)\n\n    preds_components = []\n    weights = []\n\n    # XGBoost\n    test_pred_x = None\n    if HAS_XGB:\n        try:\n            if fold == 1 and HAS_OPTUNA:\n                params_x = optuna_light_xgb(X_tr_t.values, y_tr.values, X_val_t.values, y_val.values, n_trials=12)\n            else:\n                params_x = xgb_default\n            m_x = train_xgb(X_tr_t.values, y_tr.values, X_val_t.values, y_val.values, params=params_x)\n            pv = m_x.predict(xgb.DMatrix(X_val_t.values))\n            preds_components.append(pv); weights.append(WEIGHT_XGB)\n            if X_test_t is not None:\n                test_pred_x = m_x.predict(xgb.DMatrix(X_test_t.values))\n        except Exception as e:\n            print(\"XGB fold error:\", e)\n\n    # LightGBM\n    test_pred_l = None\n    if HAS_LGB:\n        try:\n            m_l = train_lgb(X_tr_t.values, y_tr.values, X_val_t.values, y_val.values, params=lgb_default)\n            best_it = getattr(m_l, \"best_iteration\", None)\n            if best_it and best_it > 0:\n                pv = m_l.predict(X_val_t.values, num_iteration=best_it)\n            else:\n                pv = m_l.predict(X_val_t.values)\n            preds_components.append(pv); weights.append(WEIGHT_LGB)\n            if X_test_t is not None:\n                test_pred_l = m_l.predict(X_test_t.values, num_iteration=best_it) if best_it else m_l.predict(X_test_t.values)\n        except Exception as e:\n            print(\"LGB fold error:\", e)\n\n    # CatBoost\n    test_pred_c = None\n    if HAS_CAT:\n        try:\n            m_c = train_cat(X_tr_t, y_tr.values, X_val_t, y_val.values, params=cat_default)\n            pv = m_c.predict_proba(X_val_t)[:,1]\n            preds_components.append(pv); weights.append(WEIGHT_CAT)\n            if X_test_t is not None:\n                test_pred_c = m_c.predict_proba(X_test_t)[:,1]\n        except Exception as e:\n            print(\"CAT fold error:\", e)\n\n    if len(preds_components) == 0:\n        raise RuntimeError(\"No models trained this fold. Install xgboost, lightgbm or catboost.\")\n\n    weights = np.array(weights, dtype=float)\n    weights = weights / weights.sum()\n\n    fold_preds = np.zeros_like(preds_components[0], dtype=float)\n    for p,w in zip(preds_components, weights):\n        fold_preds += p * w\n\n    oof[val_idx] = fold_preds\n    auc_fold = roc_auc_score(y_val, fold_preds)\n    cv_scores.append(auc_fold)\n    print(f\"Fold {fold} AUC: {auc_fold:.6f}\")\n\n    # test fold preds blended\n    test_components = []\n    test_weights = []\n    if test_pred_x is not None:\n        test_components.append(test_pred_x); test_weights.append(WEIGHT_XGB)\n    if test_pred_l is not None:\n        test_components.append(test_pred_l); test_weights.append(WEIGHT_LGB)\n    if test_pred_c is not None:\n        test_components.append(test_pred_c); test_weights.append(WEIGHT_CAT)\n    if len(test_components) > 0:\n        tw = np.array(test_weights, dtype=float)\n        tw = tw / tw.sum()\n        tpred = np.zeros_like(test_components[0], dtype=float)\n        for p,w in zip(test_components, tw):\n            tpred += p * w\n        test_fold_preds.append(tpred)\n\n    # cleanup\n    del X_tr_t, X_val_t\n    gc.collect()\n\n# CV summary\nprint(\"\\nCV AUCs:\", [round(x,6) for x in cv_scores])\nprint(\"Mean CV AUC:\", np.mean(cv_scores))\noof_auc = roc_auc_score(y, oof)\nprint(\"OOF AUC:\", oof_auc)\npd.DataFrame({\"oof\":oof, \"target\":y}).to_csv(os.path.join(OUT_DIR,\"oof_preds.csv\"), index=False)\n\n# -------------------- Final full-data retrain + test predict --------------------\nprint(\"\\nRetraining on full data and producing final test predictions...\")\n\n# fit full transforms\nfull_num_imp = SimpleImputer(strategy=\"median\")\nfull_scaler = StandardScaler()\nfull_ord = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1) if len(categorical_feats)>0 else None\n\nX_full = df[features].reset_index(drop=True)\ny_full = df[TARGET_COL].reset_index(drop=True)\n\nXn_full = pd.DataFrame(full_num_imp.fit_transform(X_full[numeric_feats]), columns=numeric_feats)\nXn_full[numeric_feats] = full_scaler.fit_transform(Xn_full[numeric_feats])\nif full_ord is not None:\n    Xc_full = pd.DataFrame(full_ord.fit_transform(X_full[categorical_feats]), columns=categorical_feats)\nelse:\n    Xc_full = pd.DataFrame(index=Xn_full.index)\nX_full_trans = pd.concat([Xn_full.reset_index(drop=True), Xc_full.reset_index(drop=True)], axis=1)\n\n# transform test using full transforms - if df_test missing, we'll try sample_submission or create placeholder later\nif df_test is not None:\n    Xn_test = pd.DataFrame(full_num_imp.transform(df_test[numeric_feats]), columns=numeric_feats)\n    Xn_test[numeric_feats] = full_scaler.transform(Xn_test[numeric_feats])\n    if full_ord is not None:\n        Xc_test = pd.DataFrame(full_ord.transform(df_test[categorical_feats]), columns=categorical_feats)\n    else:\n        Xc_test = pd.DataFrame(index=Xn_test.index)\n    X_test_trans = pd.concat([Xn_test.reset_index(drop=True), Xc_test.reset_index(drop=True)], axis=1)\nelse:\n    X_test_trans = None\n\nfinal_components = []\nfinal_weights = []\n\n# XGB full\nif HAS_XGB:\n    try:\n        m_x_full = train_xgb(X_full_trans.values, y_full.values, X_full_trans.values, y_full.values, params=xgb_default)\n        if X_test_trans is not None:\n            pred_x_test = m_x_full.predict(xgb.DMatrix(X_test_trans.values))\n            final_components.append(pred_x_test); final_weights.append(WEIGHT_XGB)\n    except Exception as e:\n        print(\"XGB full retrain error:\", e)\n\n# LGB full\nif HAS_LGB:\n    try:\n        m_l_full = train_lgb(X_full_trans.values, y_full.values, X_full_trans.values, y_full.values, params=lgb_default)\n        best_it = getattr(m_l_full, \"best_iteration\", None)\n        if X_test_trans is not None:\n            pred_l_test = m_l_full.predict(X_test_trans.values, num_iteration=best_it) if best_it and best_it>0 else m_l_full.predict(X_test_trans.values)\n            final_components.append(pred_l_test); final_weights.append(WEIGHT_LGB)\n    except Exception as e:\n        print(\"LGB full retrain error:\", e)\n\n# CAT full\nif HAS_CAT:\n    try:\n        m_c_full = train_cat(X_full_trans, y_full.values, X_full_trans, y_full.values, params=cat_default)\n        if X_test_trans is not None:\n            pred_c_test = m_c_full.predict_proba(X_test_trans)[:,1]\n            final_components.append(pred_c_test); final_weights.append(WEIGHT_CAT)\n    except Exception as e:\n        print(\"CAT full retrain error:\", e)\n\n# If we have no final_components but had test_fold_preds (from CV), use averaged fold preds\nif len(final_components) == 0 and len(test_fold_preds) > 0:\n    final_preds = np.mean(np.vstack(test_fold_preds), axis=0)\n    print(\"No full retrain preds available; using averaged CV-fold test predictions.\")\nelif len(final_components) == 0:\n    final_preds = None\n    print(\"No test predictions available from models or CV. Will attempt fallback submission later.\")\nelse:\n    fw = np.array(final_weights, dtype=float)\n    fw = fw / fw.sum()\n    final_preds = np.zeros_like(final_components[0], dtype=float)\n    for p,w in zip(final_components, fw):\n        final_preds += p * w\n    final_preds = np.clip(final_preds, 0.0, 1.0)\n\n# -------------------- Robust Submission Builder --------------------\ndef find_sample_submission(input_root=INPUT_ROOT):\n    if not os.path.exists(input_root):\n        return None\n    # typical names\n    for root, dirs, files in os.walk(input_root):\n        for f in files:\n            if \"sample\" in f.lower() and \"sub\" in f.lower() and f.lower().endswith(\".csv\"):\n                return os.path.join(root, f)\n    # fallback: any csv in input root that looks like submission\n    return None\n\ndef build_submission_from_predictions(preds, test_df=None, out_name=None, id_col_candidates=ID_COL_CANDIDATES, target_col=TARGET_COL, as_int=False):\n    if preds is None:\n        raise RuntimeError(\"No prediction array provided for submission.\")\n    preds = np.asarray(preds).ravel()\n    preds = np.clip(preds, 0.0, 1.0)\n    if np.all(preds == preds[0]):\n        preds = preds + np.linspace(-1e-9, 1e-9, len(preds))\n        preds = np.clip(preds, 0.0, 1.0)\n\n    if test_df is None:\n        # placeholder using indices\n        sub = pd.DataFrame({\"id\": np.arange(len(preds)), target_col: preds})\n        print(\"WARNING: No test file / sample_submission found. Saving placeholder submission using indices as 'id'. This will likely not match the official test set.\")\n    else:\n        id_col = None\n        for c in id_col_candidates:\n            if c in test_df.columns:\n                id_col = c; break\n        if id_col is not None:\n            sub = pd.DataFrame({id_col: test_df[id_col].values, target_col: preds})\n        else:\n            # use index as id\n            sub = pd.DataFrame({\"id\": test_df.index.values, target_col: preds})\n\n    if as_int:\n        sub[target_col] = (sub[target_col] >= 0.5).astype(int)\n\n    if test_df is not None and len(preds) != len(test_df):\n        # trim/pad if mismatch small, else raise\n        if len(preds) > len(test_df):\n            preds = preds[:len(test_df)]\n            sub = sub.iloc[:len(test_df)]\n            sub[target_col] = preds\n            print(\"Trimmed predictions to match test length.\")\n        else:\n            pad_len = len(test_df) - len(preds)\n            pad_vals = np.full(pad_len, preds.mean())\n            preds = np.concatenate([preds, pad_vals])\n            sub[target_col] = preds\n            print(f\"Padded predictions by {pad_len} to match test length.\")\n\n    if sub.isnull().any().any():\n        raise ValueError(\"Submission contains NaN values. Aborting save.\")\n\n    if out_name is None:\n        out_name = f\"submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n    out_path = os.path.join(OUT_DIR, out_name)\n    sub.to_csv(out_path, index=False)\n    print(\"Saved submission to:\", out_path)\n    print(\"Submission preview:\\n\", sub.head())\n    return out_path, sub\n\n# Decide which preds to use (prefer final_preds, else averaged CV test folds, else OOF fallback)\npreds_to_use = None\nif final_preds is not None:\n    preds_to_use = final_preds\nelif len(test_fold_preds) > 0:\n    preds_to_use = np.mean(np.vstack(test_fold_preds), axis=0)\nelif 'oof' in globals():\n    print(\"No test predictions available; using OOF as placeholder predictions.\")\n    preds_to_use = oof\nelse:\n    raise RuntimeError(\"No predictions available for submission. Run training first.\")\n\n# Attempt to load explicit test_df (prefer TEST_PATH), else sample_submission skeleton\ntest_df_for_submission = None\nif df_test is not None:\n    test_df_for_submission = df_test.copy()\nelse:\n    sample_path = find_sample_submission()\n    if sample_path:\n        print(\"Found sample_submission at:\", sample_path)\n        try:\n            s = pd.read_csv(sample_path)\n            test_df_for_submission = s.copy()\n            # If sample submission typically contains only id & target, we'll use its id column for submission\n        except Exception as e:\n            print(\"Failed to read sample_submission:\", e)\n            test_df_for_submission = None\n\n# If we have a sample_submission but preds length mismatch, we'll trim/pad inside builder\nout_path, submission_df = build_submission_from_predictions(preds_to_use, test_df=test_df_for_submission, out_name=f\"submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\", target_col=TARGET_COL, as_int=AS_INTEGER_SUBMISSION)\n\nprint(\"\\n--- Done ---\")\nprint(\"Artifacts directory:\", OUT_DIR)\nprint(\"Mean CV AUC: {:.6f} | OOF AUC: {:.6f}\".format(np.mean(cv_scores) if len(cv_scores)>0 else float('nan'), oof_auc if 'oof_auc' in globals() else float('nan')))\nprint(\"Submission file:\", out_path)\nprint(\"If Kaggle rejects the submission, paste the exact error message and I will adapt the CSV format accordingly.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T04:02:35.828989Z","iopub.execute_input":"2025-10-22T04:02:35.829405Z","iopub.status.idle":"2025-10-22T04:02:42.206447Z","shell.execute_reply.started":"2025-10-22T04:02:35.829379Z","shell.execute_reply":"2025-10-22T04:02:42.205503Z"}},"outputs":[{"name":"stderr","text":"[I 2025-10-22 04:02:35,936] A new study created in memory with name: no-name-0821fbae-f550-4822-a1e7-a3737cba0803\n[I 2025-10-22 04:02:35,996] Trial 0 finished with value: 0.7321428571428571 and parameters: {'eta': 0.015355286838886862, 'max_depth': 7, 'subsample': 0.892797576724562, 'colsample_bytree': 0.7993292420985183}. Best is trial 0 with value: 0.7321428571428571.\n[I 2025-10-22 04:02:36,034] Trial 1 finished with value: 0.7388392857142857 and parameters: {'eta': 0.007979118876474874, 'max_depth': 3, 'subsample': 0.6232334448672797, 'colsample_bytree': 0.9330880728874675}. Best is trial 1 with value: 0.7388392857142857.\n[I 2025-10-22 04:02:36,088] Trial 2 finished with value: 0.7321428571428572 and parameters: {'eta': 0.03027182927734624, 'max_depth': 6, 'subsample': 0.608233797718321, 'colsample_bytree': 0.9849549260809971}. Best is trial 1 with value: 0.7388392857142857.\n","output_type":"stream"},{"name":"stdout","text":"Renamed target column: condition -> target\nNumeric features: ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'age_chol', 'age_thalach', 'chol_trestbps_ratio']\nCategorical features: ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'age_bin', 'ca_missing', 'thal_missing']\n\n=== Fold 1/10 ===\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-10-22 04:02:36,128] Trial 3 finished with value: 0.71875 and parameters: {'eta': 0.060534484680010825, 'max_depth': 4, 'subsample': 0.6727299868828402, 'colsample_bytree': 0.5917022549267169}. Best is trial 1 with value: 0.7388392857142857.\n[I 2025-10-22 04:02:36,175] Trial 4 finished with value: 0.7276785714285714 and parameters: {'eta': 0.012439367209907218, 'max_depth': 5, 'subsample': 0.7727780074568463, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 0.7388392857142857.\n[I 2025-10-22 04:02:36,212] Trial 5 finished with value: 0.7366071428571428 and parameters: {'eta': 0.03126143958203108, 'max_depth': 3, 'subsample': 0.7168578594140873, 'colsample_bytree': 0.6831809216468459}. Best is trial 1 with value: 0.7388392857142857.\n[I 2025-10-22 04:02:36,266] Trial 6 finished with value: 0.7053571428571429 and parameters: {'eta': 0.019603369861210685, 'max_depth': 6, 'subsample': 0.6798695128633439, 'colsample_bytree': 0.7571172192068059}. Best is trial 1 with value: 0.7388392857142857.\n[I 2025-10-22 04:02:36,312] Trial 7 finished with value: 0.7589285714285714 and parameters: {'eta': 0.029493012052163467, 'max_depth': 3, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.5852620618436457}. Best is trial 7 with value: 0.7589285714285714.\n[I 2025-10-22 04:02:36,428] Trial 8 finished with value: 0.7075892857142857 and parameters: {'eta': 0.0060758085133366885, 'max_depth': 7, 'subsample': 0.9862528132298237, 'colsample_bytree': 0.9041986740582306}. Best is trial 7 with value: 0.7589285714285714.\n[I 2025-10-22 04:02:36,467] Trial 9 finished with value: 0.7209821428571428 and parameters: {'eta': 0.012453219846912198, 'max_depth': 3, 'subsample': 0.8736932106048627, 'colsample_bytree': 0.7200762468698007}. Best is trial 7 with value: 0.7589285714285714.\n[I 2025-10-22 04:02:36,524] Trial 10 finished with value: 0.7366071428571428 and parameters: {'eta': 0.08675111366682721, 'max_depth': 4, 'subsample': 0.8317825395858812, 'colsample_bytree': 0.5089809378074098}. Best is trial 7 with value: 0.7589285714285714.\n[I 2025-10-22 04:02:36,571] Trial 11 finished with value: 0.71875 and parameters: {'eta': 0.005235569048187518, 'max_depth': 3, 'subsample': 0.7728145700056765, 'colsample_bytree': 0.8621587205082972}. Best is trial 7 with value: 0.7589285714285714.\n","output_type":"stream"},{"name":"stdout","text":"Optuna best params: {'eta': 0.029493012052163467, 'max_depth': 3, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.5852620618436457}\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[1]\ttraining's auc: 0.904217\tvalid_1's auc: 0.850446\nFold 1 AUC: 0.758929\n\n=== Fold 2/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[31]\ttraining's auc: 0.942553\tvalid_1's auc: 0.866071\nFold 2 AUC: 0.875000\n\n=== Fold 3/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[20]\ttraining's auc: 0.943626\tvalid_1's auc: 0.9375\nFold 3 AUC: 0.915179\n\n=== Fold 4/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[52]\ttraining's auc: 0.96014\tvalid_1's auc: 0.754464\nFold 4 AUC: 0.732143\n\n=== Fold 5/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[30]\ttraining's auc: 0.946138\tvalid_1's auc: 0.9375\nFold 5 AUC: 0.933036\n\n=== Fold 6/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[32]\ttraining's auc: 0.942186\tvalid_1's auc: 0.964286\nFold 6 AUC: 0.946429\n\n=== Fold 7/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[2]\ttraining's auc: 0.901169\tvalid_1's auc: 0.917411\nFold 7 AUC: 0.830357\n\n=== Fold 8/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[8]\ttraining's auc: 0.926887\tvalid_1's auc: 0.956731\nFold 8 AUC: 0.975962\n\n=== Fold 9/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[21]\ttraining's auc: 0.93414\tvalid_1's auc: 0.995192\nFold 9 AUC: 0.966346\n\n=== Fold 10/10 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[3]\ttraining's auc: 0.912774\tvalid_1's auc: 0.978365\nFold 10 AUC: 0.961538\n\nCV AUCs: [0.758929, 0.875, 0.915179, 0.732143, 0.933036, 0.946429, 0.830357, 0.975962, 0.966346, 0.961538]\nMean CV AUC: 0.8894917582417582\nOOF AUC: 0.8891423357664234\n\nRetraining on full data and producing final test predictions...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[458]\ttraining's auc: 1\tvalid_1's auc: 1\nNo test predictions available from models or CV. Will attempt fallback submission later.\nNo test predictions available; using OOF as placeholder predictions.\nWARNING: No test file / sample_submission found. Saving placeholder submission using indices as 'id'. This will likely not match the official test set.\nSaved submission to: artifacts_kaggle/submission_20251022_040242.csv\nSubmission preview:\n    id    target\n0   0  0.514459\n1   1  0.427672\n2   2  0.415690\n3   3  0.425827\n4   4  0.421955\n\n--- Done ---\nArtifacts directory: artifacts_kaggle\nMean CV AUC: 0.889492 | OOF AUC: 0.889142\nSubmission file: artifacts_kaggle/submission_20251022_040242.csv\nIf Kaggle rejects the submission, paste the exact error message and I will adapt the CSV format accordingly.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}